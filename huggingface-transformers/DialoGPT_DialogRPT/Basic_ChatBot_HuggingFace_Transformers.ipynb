{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model_score = \"microsoft/DialogRPT-updown\"   \n",
    "tokenizer_rpt = AutoTokenizer.from_pretrained(model_score)\n",
    "model_rpt = AutoModelForSequenceClassification.from_pretrained(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(user_input:str,\n",
    "                   context: List[str] = []) -> List[str]:\n",
    "    \"\"\"Your code here\"\"\"\n",
    "    \n",
    "    user_input = tokenizer.encode(user_input, tokenizer.eos_token, return_tensors='pt')\n",
    "    \n",
    "    beam_outputs = model.generate(\n",
    "        user_input, \n",
    "        max_length=60, \n",
    "        num_beams=5, \n",
    "        no_repeat_ngram_size=2, \n",
    "        num_return_sequences=5, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    beam_output = []\n",
    "    for step in range(5):\n",
    "        m = format(tokenizer.decode(beam_outputs[:, user_input.shape[-1]:][step], skip_special_tokens=True))\n",
    "        beam_output.append(m)\n",
    "    \n",
    "    return beam_output, beam_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(context:List[str],\n",
    "               user_input:str,\n",
    "               candidates: List[str]) -> List[float]:\n",
    "    \n",
    "    \"\"\"Your code here\"\"\"\n",
    "    \n",
    "    result = model_rpt(context, return_dict=True)\n",
    "    candidate_scores = torch.sigmoid(result.logits)\n",
    "\n",
    "    \n",
    "    return candidate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    #init the chat with 0 context\n",
    "    context = []\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        #get user input\n",
    "        user_input = input('user > ')\n",
    "        \n",
    "        candidates, output = get_candidates(user_input)\n",
    "        print(candidates)\n",
    "        \n",
    "        scores = get_scores(output, user_input, candidates)\n",
    "        \n",
    "        print(scores)\n",
    "        \n",
    "        response = candidates[torch.argmax(scores)]\n",
    "\n",
    "        print('Bot >', response)\n",
    "        \n",
    "        context.extend([user_input,response])\n",
    "        \n",
    "        print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user > hey! Good Morning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good morning!', 'Good morning to you!', 'Good Morning!', 'Good morning!!', 'Good morning']\n",
      "tensor([[0.1799],\n",
      "        [0.2011],\n",
      "        [0.1940],\n",
      "        [0.1594],\n",
      "        [0.2060]], grad_fn=<SigmoidBackward>)\n",
      "Bot > Good morning\n",
      "['hey! Good Morning!', 'Good morning']\n",
      "user > what did you do last night?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm not sure what you mean.\", \"I'm not sure what you mean?\", 'Nothing, I just woke up.', \"I'm not sure what you mean by that.\", \"I'm not sure what you're asking.\"]\n",
      "tensor([[0.3189],\n",
      "        [0.3189],\n",
      "        [0.3130],\n",
      "        [0.3014],\n",
      "        [0.3852]], grad_fn=<SigmoidBackward>)\n",
      "Bot > I'm not sure what you're asking.\n",
      "['hey! Good Morning!', 'Good morning', 'what did you do last night?', \"I'm not sure what you're asking.\"]\n",
      "user > what did you eat last night?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I don't know what you're talking about.\", \"I don't know what you mean.\", \"I don't know what you mean by that.\", \"I don't know what you're asking me.\", \"I don't know what you're talking about, but I'm pretty sure I ate a lot of pizza.\"]\n",
      "tensor([[0.4187],\n",
      "        [0.3205],\n",
      "        [0.2743],\n",
      "        [0.4029],\n",
      "        [0.4762]], grad_fn=<SigmoidBackward>)\n",
      "Bot > I don't know what you're talking about, but I'm pretty sure I ate a lot of pizza.\n",
      "['hey! Good Morning!', 'Good morning', 'what did you do last night?', \"I'm not sure what you're asking.\", 'what did you eat last night?', \"I don't know what you're talking about, but I'm pretty sure I ate a lot of pizza.\"]\n",
      "user > I like pizza too\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pizza is good.', 'Pizza is the best.', 'Pizza is awesome.', 'Pizza is the best!', 'Pizza is the best pizza.']\n",
      "tensor([[0.2201],\n",
      "        [0.2703],\n",
      "        [0.2353],\n",
      "        [0.2703],\n",
      "        [0.3831]], grad_fn=<SigmoidBackward>)\n",
      "Bot > Pizza is the best pizza.\n",
      "['hey! Good Morning!', 'Good morning', 'what did you do last night?', \"I'm not sure what you're asking.\", 'what did you eat last night?', \"I don't know what you're talking about, but I'm pretty sure I ate a lot of pizza.\", 'I like pizza too', 'Pizza is the best pizza.']\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
